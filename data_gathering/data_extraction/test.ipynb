{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:19.529186692Z",
     "start_time": "2023-06-06T06:34:18.631742248Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "from tree_modification import simplify_body, textify_simplified_head\n",
    "from utils import get_binary_dicts_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# web_pages = os.listdir(\"web_pages/all_domains/pages\")\n",
    "web_pages_products = os.listdir(\"../../../../studia/master-thesis/data/02_intermediate/web_pages/products\")\n",
    "web_pages_other = os.listdir(\"../../../../studia/master-thesis/data/02_intermediate/web_pages/other\")\n",
    "# len(web_pages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.701578218Z",
     "start_time": "2023-06-06T06:34:19.530291770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "web_pages_products = random.sample(web_pages_products, len(web_pages_products) // 10)\n",
    "web_pages_other = random.sample(web_pages_other, len(web_pages_other) // 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.732981996Z",
     "start_time": "2023-06-06T06:34:20.703312838Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(20039, 7418)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(web_pages_products), len(web_pages_other)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.774624617Z",
     "start_time": "2023-06-06T06:34:20.705731841Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# web_pages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.805579652Z",
     "start_time": "2023-06-06T06:34:20.706608699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open(\"./dataset_creation_config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "tags_to_include = set(config[\"tags\"])\n",
    "text_formatting_tags = set(config[\"text_formatting_tags\"])\n",
    "meta_values = config[\"meta_values\"]\n",
    "abbreviations = config[\"abbreviations\"]\n",
    "(\n",
    "    available_tags_binary_dict,\n",
    "    available_attributes_values_binary_dict,\n",
    ") = get_binary_dicts_templates(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.807251477Z",
     "start_time": "2023-06-06T06:34:20.707468232Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from markdownify import MarkdownConverter, ATX, BACKSLASH\n",
    "from bs4 import Tag\n",
    "\n",
    "# Create shorthand method for conversion\n",
    "def create_md(**options):\n",
    "    converter = MarkdownConverter(**options)\n",
    "\n",
    "    def md(soup: Tag):\n",
    "        return converter.convert_soup(soup)\n",
    "\n",
    "    return md"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.807983906Z",
     "start_time": "2023-06-06T06:34:20.707854890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "md = create_md(heading_style=ATX, newline_style=BACKSLASH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.808503826Z",
     "start_time": "2023-06-06T06:34:20.708209116Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def compare_strings_line_by_line(str1, str2):\n",
    "    str1_lines = [elem.strip() for elem in str1.strip().splitlines()]\n",
    "    str2_lines = [elem.strip() for elem in str2.strip().splitlines()]\n",
    "\n",
    "    for i in range(len(str1_lines)):\n",
    "        if str1_lines[i] != str2_lines[i]:\n",
    "            return False\n",
    "\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T22:03:51.445631442Z",
     "start_time": "2023-06-05T22:03:51.339821069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def compare_files(web_pages, path):\n",
    "    different_files = []\n",
    "\n",
    "    for web_page in web_pages:\n",
    "        html = None\n",
    "        with open(f\"{path}/{web_page}\", \"r\") as f:\n",
    "            html = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        simplified_soup_body_1 = simplify_body(\n",
    "            soup=soup.body,\n",
    "            text_formatting_tags=text_formatting_tags,\n",
    "            tags_to_include=tags_to_include\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        simplified_soup_body_2 = simplify_body(\n",
    "            soup=soup.body,\n",
    "            text_formatting_tags=text_formatting_tags,\n",
    "            tags_to_include=tags_to_include,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            if not compare_strings_line_by_line(simplified_soup_body_1.prettify(), simplified_soup_body_2.prettify()):\n",
    "                different_files.append(web_page)\n",
    "        except Exception:\n",
    "            different_files.append(web_page)\n",
    "\n",
    "    return different_files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T10:34:43.506988278Z",
     "start_time": "2023-06-05T10:34:43.506658406Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guzik/miniconda3/envs/global_data/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cchardet\n",
    "\n",
    "different_files_dict = {\n",
    "    'products': compare_files(web_pages_products, '../../../../studia/master-thesis/data/02_intermediate/web_pages/products'),\n",
    "    'other': compare_files(web_pages_other, '../../../../studia/master-thesis/data/02_intermediate/web_pages/other')\n",
    "}\n",
    "\n",
    "with open(\"different_files.json\", \"w\") as file:\n",
    "    json.dump(different_files_dict, file, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T14:07:05.189331460Z",
     "start_time": "2023-06-05T10:34:43.506864898Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from time import time\n",
    "import cchardet\n",
    "\n",
    "simplify_lxml_times = []\n",
    "simplify_html_parser_times = []\n",
    "md_lxml_times = []\n",
    "md_html_parser_times = []\n",
    "\n",
    "def get_times(web_pages, path):\n",
    "    for web_page in web_pages:\n",
    "        html = None\n",
    "        with open(f\"{path}/{web_page}\", \"r\") as f:\n",
    "            html = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # simplified_soup_head_text = textify_simplified_head(soup=soup.head, meta_acceptable_values=meta_values)\n",
    "\n",
    "        try:\n",
    "            start = time()\n",
    "            simplified_soup_body = simplify_body(\n",
    "                soup=soup.body,\n",
    "                text_formatting_tags=text_formatting_tags,\n",
    "                tags_to_include=tags_to_include,\n",
    "            )\n",
    "            end = time()\n",
    "            simplify_lxml_times.append(end - start)\n",
    "\n",
    "            start = time()\n",
    "            simplified_body_text = md(simplified_soup_body)\n",
    "            end = time()\n",
    "            md_lxml_times.append(end - start)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # full_page_text = f\"{simplified_soup_head_text}\\n{simplified_body_text}\"\n",
    "        #\n",
    "        # with open(f\"web_pages/all_domains/results_lxml/{web_page}\", \"w\") as f:\n",
    "        #     f.write(full_page_text)\n",
    "        #\n",
    "        # # save simplified soup to file\n",
    "        # with open(f\"web_pages/all_domains/simplified_lxml/{web_page}\", \"w\") as file:\n",
    "        #     file.write(simplified_soup_body.prettify())\n",
    "\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            start = time()\n",
    "            simplified_soup_body = simplify_body(\n",
    "                soup=soup.body,\n",
    "                text_formatting_tags=text_formatting_tags,\n",
    "                tags_to_include=tags_to_include,\n",
    "            )\n",
    "            end = time()\n",
    "            simplify_html_parser_times.append(end - start)\n",
    "\n",
    "            start = time()\n",
    "            simplified_body_text = html2text.html2text(simplified_soup_body.prettify())\n",
    "            end = time()\n",
    "            md_html_parser_times.append(end - start)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # full_page_text = f\"{simplified_soup_head_text}\\n{simplified_body_text}\"\n",
    "\n",
    "        # with open(f\"web_pages/all_domains/results_html_parser/{web_page}\", \"w\") as f:\n",
    "        #     f.write(full_page_text)\n",
    "        #\n",
    "        # with open(f\"web_pages/all_domains/simplified_html_parser/{web_page}\", \"w\") as file:\n",
    "        #     file.write(simplified_soup_body.prettify())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T06:34:20.808943820Z",
     "start_time": "2023-06-06T06:34:20.709255712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guzik/miniconda3/envs/global_data/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "get_times(web_pages_products, '../../../../studia/master-thesis/data/02_intermediate/web_pages/products')\n",
    "get_times(web_pages_other, '../../../../studia/master-thesis/data/02_intermediate/web_pages/other')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:29:12.974881539Z",
     "start_time": "2023-06-06T06:34:20.709682669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lxml simplify time: 0.22364103031266122\n",
      "html parser simplify time: 0.12403747533698357\n",
      "lxml md time: 0.001984712375884538\n",
      "html parser md time: 0.020920258822169198\n"
     ]
    }
   ],
   "source": [
    "print(f\"lxml simplify time: {sum(simplify_lxml_times)/len(simplify_lxml_times)}\")\n",
    "print(f\"html parser simplify time: {sum(simplify_html_parser_times)/len(simplify_html_parser_times)}\")\n",
    "print(f\"lxml md time: {sum(md_lxml_times)/len(md_lxml_times)}\")\n",
    "print(f\"html parser md time: {sum(md_html_parser_times)/len(md_html_parser_times)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T10:29:13.021561405Z",
     "start_time": "2023-06-06T10:29:13.017974851Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Old results\n",
    "\n",
    "lxml simplify time: 0.18664660945099995\n",
    "html parser simplify time: 0.10447529040329413\n",
    "lxml md time: 0.0016778466235480365\n",
    "html parser md time: 0.017234264529081146"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for web_page in os.listdir(\"web_pages/all_domains/broken_pages\"):\n",
    "    html = None\n",
    "    with open(f\"web_pages/all_domains/broken_pages/{web_page}\", \"r\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    simplified_soup_head_text = textify_simplified_head(soup=soup.head, meta_acceptable_values=meta_values)\n",
    "\n",
    "    try:\n",
    "        start = time()\n",
    "        simplified_soup_body = simplify_body(\n",
    "            soup=soup.body,\n",
    "            text_formatting_tags=text_formatting_tags,\n",
    "            tags_to_include=tags_to_include,\n",
    "        )\n",
    "        end = time()\n",
    "        simplify_lxml_times.append(end - start)\n",
    "\n",
    "        start = time()\n",
    "        simplified_body_text = md(simplified_soup_body)\n",
    "        end = time()\n",
    "        md_lxml_times.append(end - start)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # full_page_text = f\"{simplified_soup_head_text}\\n{simplified_body_text}\"\n",
    "    #\n",
    "    # with open(f\"web_pages/all_domains/results_lxml/{web_page}\", \"w\") as f:\n",
    "    #     f.write(full_page_text)\n",
    "    #\n",
    "    # # save simplified soup to file\n",
    "    # with open(f\"web_pages/all_domains/simplified_lxml/{web_page}\", \"w\") as file:\n",
    "    #     file.write(simplified_soup_body.prettify())\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        start = time()\n",
    "        simplified_soup_body = simplify_body(\n",
    "            soup=soup.body,\n",
    "            text_formatting_tags=text_formatting_tags,\n",
    "            tags_to_include=tags_to_include,\n",
    "        )\n",
    "        end = time()\n",
    "        simplify_html_parser_times.append(end - start)\n",
    "\n",
    "        start = time()\n",
    "        simplified_body_text = html2text.html2text(simplified_soup_body.prettify())\n",
    "        end = time()\n",
    "        md_html_parser_times.append(end - start)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # full_page_text = f\"{simplified_soup_head_text}\\n{simplified_body_text}\"\n",
    "\n",
    "    # with open(f\"web_pages/all_domains/results_html_parser/{web_page}\", \"w\") as f:\n",
    "    #     f.write(full_page_text)\n",
    "    #\n",
    "    # with open(f\"web_pages/all_domains/simplified_html_parser/{web_page}\", \"w\") as file:\n",
    "    #     file.write(simplified_soup_body.prettify())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# import filecmp\n",
    "#\n",
    "# for web_page in web_pages:\n",
    "#     res = filecmp.cmp(f\"web_pages/all_domains/simplified/{web_page}\", f\"web_pages/all_domains/simpl_html_parser/{web_page}\")\n",
    "#     if not res:\n",
    "#         print(res, web_page)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T08:49:44.997763360Z",
     "start_time": "2023-06-03T08:49:44.945415357Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# with open(\"different_files.json\", \"r\") as file:\n",
    "#     different_files_dict = json.load(file)\n",
    "#\n",
    "# different_files_products = different_files_dict['products']\n",
    "# different_files_other = different_files_dict['other']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:11:48.438583774Z",
     "start_time": "2023-06-05T08:11:48.435060712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from shutil import copyfile\n",
    "#\n",
    "# for file in different_files_products:\n",
    "#     copyfile(f\"../../../../studia/master-thesis/data/02_intermediate/web_pages/products/{file}\", f\"web_pages/all_domains/broken_pages/{file}\")\n",
    "#\n",
    "# for file in different_files_other:\n",
    "#     copyfile(f\"../../../../studia/master-thesis/data/02_intermediate/web_pages/other/{file}\", f\"web_pages/all_domains/broken_pages/{file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T08:15:43.417401260Z",
     "start_time": "2023-06-05T08:15:43.407698994Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
